Paper: LEARNING REPRESENTATIONS FROM EEG WITH DEEP RECURRENT-CONVOLUTIONAL NEURAL NETWORKS

Aimed at extracting features whilst accounting for inter-subject invariants.

A deep recurrent convectional neural network trained on EEG movie.

ConvNets are used for extracting spatial and spectral invariants (for each frame).

Why are LSTMs used for extracting temporal features?

Most important features are from the frequency domain.
 - Usually studied using a spectrogram of the signal.
 - EEG has an additional spatial domain.

Fast Fourier Transform (FFT) is performed on the time series for each trial to estimate the power spectrum of the signal.
 - Oscillatory cortical activity related to memory operations primarily exists in three frequency bands of theta (4-7Hz), alpha (8-13Hz), and beta (13-30Hz)
 - Sum of squared absolute values within each of the three frequency bands was computed and used as separate measurement for each electrode.
 - TODO: What are the frequency bands that we are concerned with.

Forming a feature vector ignores the inherent structure of the data in space, frequency and time.
 - Instead, transform the measurements into a 2D image to preserve the spatial structure; and
 - Use multiple colour channels to represent the spectral dimension; finally
 - Use the sequence of images derived from consecutive time windows to account for temporal evolutions in brain activity.

Width and height of the image represent the spatial distribution of activities over the cortex.

The EEG electrodes are distributed over the scalp in a three-dimensional space. In order to trans- form the spatially
distributed activity maps as 2-D images, we need to first project the location of electrodes from a 3-dimensional space onto a 2-D surface.
 - Should also preserve the relative distance between neighboring electrodes.
 - Use the Azimuthal Equidistant Projection (AEP) also known as Polar Projection.
 - A drawback of this method is that the distances between the points on the map are only preserved with respect to a
 single point (the center point) and therefore the relative distances between all pairs of electrodes will not be exactly preserved.
    - TODO: Is there any way to overcome this? Use a CNN on 3D images?
 - Applying AEP to 3-D electrode locations, we obtain 2-D projected locations of electrodes.
 - apply Clough- Tocher scheme (Alfeld, 1984) for interpolating the scattered power measurements over the scalp and for
  estimating the values in-between the electrodes over a 32 × 32 mesh.
 - This procedure is repeated for each frequency band of interest, resulting in three topographical activity maps corresponding to each frequency band.
    - TODO: What are the main frequency bands of interest?
 - The three spatial maps are then merged together to form an image with three (color) channels. (Input for the CNN).

Novelty resides in transforming raw EEG into sequence of images, or frames (EEG ”movie”), combined with recurrent-convolutional
network architecture applied on top of such transformed EEG data

ARCHITECTURE:

ARCHITECTURE STEPS / IMPLEMENTATION: TODO: Maybe more than one channel would be used?
 - (1) EEG time series from multiple locations are acquired;
 - (2) spectral power within three prominent frequency bands is extracted for each location and used to form topographical maps for each time frame (image);
 - (3) sequence of topographical maps are combined to form a sequence of 3-channel images which are fed into a recurrent-convolutional network for representation learning and classification.

ConvNets were used to deal with variations in space and frequency domains due to their ability to learn good two-dimensional representation of the data.
 - Wherever needed, the extracted representa- tions were fed into another layer to account for temporal variations in the data.

We evaluated various types of layers used for extracting temporal patterns, including convolutional and recurrent layers.
 - The following two approaches were used for state classification:

 - SINGLE-FRAME APPROACH:
    - A single image was constructed from spectral measurements over the complete trial duration;
        - EEG image was generated by applying FFT on the whole trial du- ration (3.5 seconds);
            - Purpose of this approach was to find the optimized ConvNet configuration;
                - First studied a simplified version of the problem by computing the average activity over the complete duration of trial.
                For this, we computed all power features over the whole duration of trial.
        - Following this procedure, EEG recording for each trial was reduced to a single multi-channel image;
        - Essentially, configuration A involves only two convolutional layers (Conv3-32) stacked together, followed by maxpool layer;
        - Configuration B adds on top of architecture A two more convolutional layers (Conv3-64), followed by another maxpool;
        - Configuration C adds one more convolutional layer (Conv3-128) followed by maxpool;
        - Configuration D differs from C by using 4 rather than 2 Conv3-32 convolutional layers at the beginning.
        - Finally, a fully-connected layer with 512 nodes (FC-512) is added on top of all these architectures, followed by softmax as the last layer;
    - The constructured image was then used as input to the ConvNet;

 - MULTI-FRAME APPROACH (adopted the best performing ConvNet architecture from single frame approach for each frame):
    - Divide each trial into 0.5 second windows and constructed an image over each time window, delivering 7 frames per trial.
    - The sequence of images was then used as input data to the recurrent-convolutional network;
    - In order to reduce the number of parameters in the network, all ConvNets share parameters across frames.
    - Outputs of all ConvNets are reshaped as sequential frames and used to investigate temporal sequence in maps.
    - THREE APPROACHES WERE EVALUATED FOR EXTRACTING TEMPORAL INFORMATION FORM SEQUENCE OF ACTIVITY MAPS (Inspired by video classification);
        - (1) MAX POOLING OVER TIME;
            - This model performs max-pooling over ConvNet outputs across time frames. While representations found from this
            model preserve spatial location, they are nonetheless order invariant.
        - (2) TEMPORAL EVOLUTION;
            - This model applies a 1-D convolution to ConvNet outputs across time frames. We evaluated two models consisting of 16 and 32 kernels of size 3 with stride of 1 frame.
            Kernels capture distinct temporal patterns across multiple frames.
        - (3) LSTM (Long Short-Term Memory);
            - Given the dynamic nature of neural responses and, consequently, of EEG data, recurrent neural networks (RNN) appear to be a reasonable choice for modeling temporal evolution of brain activity.
            - LSTM is an RNN with improved memory. It uses memory cells with an internal memory and gated inputs/outputs which have shown to be more efficient in capturing long-term dependencies.
            - Experimented with up to two LSTM layers and various number of memory cells in each layer and obtained the best results with one layer consisting of 128 cells.
            - We adopted LSTM to capture temporal evolution in sequences of ConvNet activations. Since brain activity is a temporally dynamic process, variations between frames may contain additional information about the underlying mental state.
    - Finally, the outputs from the last layer are fed to a fully connected layer with 512 hidden units followed by a four-way softmax layer.
    - Number of neurons in the fully connected layer relatively low to control the total number of parameters in the network.
    - 50% dropout was used on the last two fully connected layers.

CONVNET ARCHITECTURE:
 - An architecture mimicking the VGG network used in Imagenet classification challenge;
    - VGG architecture requires fewer epochs to converge due to implicit regularization imposed by greater depth and smaller convolution filter sizes.

TRAINING:
Training is carried out by optimizing the cross-entropy loss function;
Weight sharing in ConvNets results in vastly different gradients in different layers and for this reason a smaller learning rate is usually used when applying SGD.

Trained the recurrent-convolutional network with Adam algorithm with a learning factor of 10^(−3), and decay rate of first and sec- ond moments as 0.9 and 0.999 respectively.
 - Adam has been shown to achieve competitively fast convergence rates when used for training ConvNets as well as multi-layer neural networks.

Batch size was set to 20.

The large number of parameters existing in our network made it susceptible to overfitting.
 - We adopted several measures to address the issue.
    - Dropout with a probability of 0.5 was used in all fully connected layers;
        - Dropout regularization has proved to be an effective method for reducing the overfitting in deep neural networks with millions of parameters
    - Used early stopping by monitoring model’s performance over a randomly selected validation set;

BASELINE METHODS
SVM:
RANDOM FORREST:
LOGISTIC REGRESSION:
DEEP BELIEF NETWORK:

EXPERIMENTS ON AN EEG DATASET
Every individual has a different cognitive processing capacity which causally determines his/her ability in performing mental tasks.
Human brain consists of numerous networks responsible for specialized tasks, many of them rely on more basic functional networks like working memory.

EEG was recorded as fifteen participants (eight female) performed a standard working memory experiment.
In brief, continuous EEG was recorded from 64 electrodes placed over the scalp at standard 10-10 locations with a sampling frequency of 500 Hz. Electrodes are placed at distances of 10% along the medial-lateral contours.

Recorded brain activity during the period which individuals retained the information in their memory (3.5 seconds) was used to recognize the amount of mental workload.

The classification task is to recognize the load level corresponding to set size (number of characters presented to the subject) from EEG recordings. Four distinct classes corresponding to load 1-4 are defined and the 2670 samples collected from 13 subjects are assigned to these four categories.

Continuous EEG was sliced offline to equal lengths of 3.5 seconds corresponding to each trial.

RESULTS:
We examined the EEG dataset from two approaches. In the first approach (single-frame) we ex- tracted the power features by applying FFT on the complete duration of each trial leading to single 3-channel image corresponding to each trial.
The second approach included dividing each trial to multiple time windows and extracting power features for each window separately leading to conser- vation of temporal information rather than averaging them out into single slice of activity map.

SINGLE FRAME CLASSIFICATION RESULTS:
 - We evaluated various configurations with different number of convolution and maxpool layers.
 - We followed the VGG architecture for selection of number of filters in each layer and grouping convolution layers with small receptive fields.
 - ConvNet based architectures to be superior to our baseline methods.
 - Most of the network parameters lie in the last two layers (fully connected and softmax) containing approximately 1 million parameters.
 - In VGG style network, the number of filters in each layer is selected in a way that size of the output remains the same after each stack (filter size × number of kernels).

The differences between topology-preserving and non-topology-preserving projections were mostly evident on the peripheral parts of the projected image.
In our experiments we observed slight improvement of classification error in using topology preserving projection over non-equidistant flat- tening projection (∼0.6%).
However, this observation could be dependent on the particular dataset and requires further exploration to conclude.
 - TODO: Generate images using a simple orthographic projection (onto the z=0 plane); maybe this will have a different outcome?

Using the equidistant projection approach helps with the interpretability of images and feature maps when visualizing the data.
 - Our claim is that mapping EEG data into a 2D image (specially with equidistant projections) leads to considerably better classification of cognitive load levels as compared to standard, non-spatial approaches that treat EEG simply as a collection of time series.

MULTI FRAME CLASSIFICATION RESULTS:
For the multi-frame classification, we used ConvNet with architecture D from previous step and applied it on each frame.
Using temporal convolution and LSTM significantly improved the classification accuracy.

For the model with temporal convolution, we found the network consisting of 32 kernels to outperform the one with 16 kernels.
 - A closer look at the accuracies derived for each individual, reveals that while both methods are achieving close to perfect classification accuracies for eight of participants, most of the differences originated from differences in accuracy for the remaining five individuals.
 - This observation motivated us to use a combination of temporal convolution and LSTM structures together in single structure which led to our best results on the dataset.

Our approach does not directly operate on raw EEG time- series, we drastically reduced the amount of required data by manually extracting power features from EEG.
Discovering complex temporal relationships such as those related to spectral properties in time-series using neural networks, is still an open question which has not been fully addressed.

ConvNets attain translation invariance through maxpooling which is a downsampling procedure in nature.
While this helps with creating invariant (with respect to space and frequency) feature maps in the deeper layers of ConvNet, it might also hurt the performance if the feature map size is reduced to a degree in which the regional activities cannot be distinguished from each other.
There is a trade-off between the degree of abstraction realized through layers of convolution and maxpooling and the level of detail kept in the feature maps.

ConvNets learn stack of filters which produce nonlinear feature maps maximizing the classification accuracy.
When trained on a pool of data containing multiple individuals, the network extracts features that are maximally informative considering the variability in the training set.

Performance of ConvNet+Maxpool is lower than ConvNet in single-frame setup.
Tem- poral maxpool selects the highest activation across the frames whereas features extracted in the single-frame approach are similar to average values over multiple frames.

Choosing the maximum value over multiple time frames is not necessarily the best practice when dealing with brain activity time series as it will potentially ignore the periods of inactivation in some cortical regions.
 - This effect partially observable when computing the average of activities over all the frames;
 - It also partially explains lower classification errors when temporal dynamic models (1D-conv and LSTM) are added to the network.
