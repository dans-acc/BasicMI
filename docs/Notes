Paper: LEARNING REPRESENTATIONS FROM EEG WITH DEEP RECURRENT-CONVOLUTIONAL NEURAL NETWORKS

Aimed at extracting features whilst accounting for inter-subject invariants.

A deep recurrent convectional neural network trained on EEG movie.

ConvNets are used for extracting spatial and spectral invariants (for each frame).

Why are LSTMs used for extracting temporal features?

Most important features are from the frequency domain.
 - Usually studied using a spectrogram of the signal.
 - EEG has an additional spatial domain.

Fast Fourier Transform (FFT) is performed on the time series for each trial to estimate the power spectrum of the signal.
 - Oscillatory cortical activity related to memory operations primarily exists in three frequency bands of theta (4-7Hz), alpha (8-13Hz), and beta (13-30Hz)
 - Sum of squared absolute values within each of the three frequency bands was computed and used as separate measurement for each electrode.
 - TODO: What are the frequency bands that we are concerned with.

Forming a feature vector ignores the inherent structure of the data in space, frequency and time.
 - Instead, transform the measurements into a 2D image to preserve the spatial structure; and
 - Use multiple colour channels to represent the spectral dimension; finally
 - Use the sequence of images derived from consecutive time windows to account for temporal evolutions in brain activity.

Width and height of the image represent the spatial distribution of activities over the cortex.

The EEG electrodes are distributed over the scalp in a three-dimensional space. In order to trans- form the spatially
distributed activity maps as 2-D images, we need to first project the location of electrodes from a 3-dimensional space onto a 2-D surface.
 - Should also preserve the relative distance between neighboring electrodes.
 - Use the Azimuthal Equidistant Projection (AEP) also known as Polar Projection.
 - A drawback of this method is that the distances between the points on the map are only preserved with respect to a
 single point (the center point) and therefore the relative distances between all pairs of electrodes will not be exactly preserved.
    - TODO: Is there any way to overcome this? Use a CNN on 3D images?
 - Applying AEP to 3-D electrode locations, we obtain 2-D projected locations of electrodes.
 - apply Clough- Tocher scheme (Alfeld, 1984) for interpolating the scattered power measurements over the scalp and for
  estimating the values in-between the electrodes over a 32 × 32 mesh.
 - This procedure is repeated for each frequency band of interest, resulting in three topographical activity maps corresponding to each frequency band.
    - TODO: What are the main frequency bands of interest?
 - The three spatial maps are then merged together to form an image with three (color) channels. (Input for the CNN).

Novelty resides in transforming raw EEG into sequence of images, or frames (EEG ”movie”), combined with recurrent-convolutional
network architecture applied on top of such transformed EEG data

ARCHITECTURE:

ARCHITECTURE STEPS / IMPLEMENTATION: TODO: Maybe more than one channel would be used?
 - (1) EEG time series from multiple locations are acquired;
 - (2) spectral power within three prominent frequency bands is extracted for each location and used to form topographical maps for each time frame (image);
 - (3) sequence of topographical maps are combined to form a sequence of 3-channel images which are fed into a recurrent-convolutional network for representation learning and classification.

ConvNets were used to deal with variations in space and frequency domains due to their ability to learn good two-dimensional representation of the data.
 - Wherever needed, the extracted representa- tions were fed into another layer to account for temporal variations in the data.

We evaluated various types of layers used for extracting temporal patterns, including convolutional and recurrent layers.
 - The following two approaches were used for state classification:

 - SINGLE-FRAME APPROACH:
    - A single image was constructed from spectral measurements over the complete trial duration;
        - EEG image was generated by applying FFT on the whole trial du- ration (3.5 seconds);
            - Purpose of this approach was to find the optimized ConvNet configuration;
                - First studied a simplified version of the problem by computing the average activity over the complete duration of trial.
                For this, we computed all power features over the whole duration of trial.
        - Following this procedure, EEG recording for each trial was reduced to a single multi-channel image;
        - Essentially, configuration A involves only two convolutional layers (Conv3-32) stacked together, followed by maxpool layer;
        - Configuration B adds on top of architecture A two more convolutional layers (Conv3-64), followed by another maxpool;
        - Configuration C adds one more convolutional layer (Conv3-128) followed by maxpool;
        - Configuration D differs from C by using 4 rather than 2 Conv3-32 convolutional layers at the beginning.
        - Finally, a fully-connected layer with 512 nodes (FC-512) is added on top of all these architectures, followed by softmax as the last layer;
    - The constructured image was then used as input to the ConvNet;

 - MULTI-FRAME APPROACH (adopted the best performing ConvNet architecture from single frame approach for each frame):
    - Divide each trial into 0.5 second windows and constructed an image over each time window, delivering 7 frames per trial.
    - The sequence of images was then used as input data to the recurrent-convolutional network;
    - In order to reduce the number of parameters in the network, all ConvNets share parameters across frames.
    - Outputs of all ConvNets are reshaped as sequential frames and used to investigate temporal sequence in maps.
    - THREE APPROACHES WERE EVALUATED FOR EXTRACTING TEMPORAL INFORMATION FORM SEQUENCE OF ACTIVITY MAPS (Inspired by video classification);
        - (1) MAX POOLING OVER TIME;
            - This model performs max-pooling over ConvNet outputs across time frames. While representations found from this
            model preserve spatial location, they are nonetheless order invariant.
        - (2) TEMPORAL EVOLUTION;
            - This model applies a 1-D convolution to ConvNet outputs across time frames. We evaluated two models consisting of 16 and 32 kernels of size 3 with stride of 1 frame.
            Kernels capture distinct temporal patterns across multiple frames.
        - (3) LSTM (Long Short-Term Memory);
            - Given the dynamic nature of neural responses and, consequently, of EEG data, recurrent neural networks (RNN) appear to be a reasonable choice for modeling temporal evolution of brain activity.
            - LSTM is an RNN with improved memory. It uses memory cells with an internal memory and gated inputs/outputs which have shown to be more efficient in capturing long-term dependencies.
            - Experimented with up to two LSTM layers and various number of memory cells in each layer and obtained the best results with one layer consisting of 128 cells.
            - We adopted LSTM to capture temporal evolution in sequences of ConvNet activations. Since brain activity is a temporally dynamic process, variations between frames may contain additional information about the underlying mental state.
    - Finally, the outputs from the last layer are fed to a fully connected layer with 512 hidden units followed by a four-way softmax layer.
    - Number of neurons in the fully connected layer relatively low to control the total number of parameters in the network.
    - 50% dropout was used on the last two fully connected layers.

CONVNET ARCHITECTURE:
 - An architecture mimicking the VGG network used in Imagenet classification challenge;
    - VGG architecture requires fewer epochs to converge due to implicit regularization imposed by greater depth and smaller convolution filter sizes.

TRAINING:
Training is carried out by optimizing the cross-entropy loss function;
Weight sharing in ConvNets results in vastly different gradients in different layers and for this reason a smaller learning rate is usually used when applying SGD.

Trained the recurrent-convolutional network with Adam algorithm with a learning factor of 10^(−3), and decay rate of first and sec- ond moments as 0.9 and 0.999 respectively.
 - Adam has been shown to achieve competitively fast convergence rates when used for training ConvNets as well as multi-layer neural networks.

Batch size was set to 20.

The large number of parameters existing in our network made it susceptible to overfitting.
 - We adopted several measures to address the issue.
    - Dropout with a probability of 0.5 was used in all fully connected layers;
        - Dropout regularization has proved to be an effective method for reducing the overfitting in deep neural networks with millions of parameters
    - Used early stopping by monitoring model’s performance over a randomly selected validation set;

BASELINE METHODS
SVM:
RANDOM FORREST:
LOGISTIC REGRESSION:
DEEP BELIEF NETWORK:

EXPERIMENTS ON AN EEG DATASET
Every individual has a different cognitive processing capacity which causally determines his/her ability in performing mental tasks.
Human brain consists of numerous networks responsible for specialized tasks, many of them rely on more basic functional networks like working memory.

EEG was recorded as fifteen participants (eight female) performed a standard working memory experiment.
In brief, continuous EEG was recorded from 64 electrodes placed over the scalp at standard 10-10 locations with a sampling frequency of 500 Hz. Electrodes are placed at distances of 10% along the medial-lateral contours.

Recorded brain activity during the period which individuals retained the information in their memory (3.5 seconds) was used to recognize the amount of mental workload.

The classification task is to recognize the load level corresponding to set size (number of characters presented to the subject) from EEG recordings. Four distinct classes corresponding to load 1-4 are defined and the 2670 samples collected from 13 subjects are assigned to these four categories.

Continuous EEG was sliced offline to equal lengths of 3.5 seconds corresponding to each trial.

RESULTS:
We examined the EEG dataset from two approaches. In the first approach (single-frame) we ex- tracted the power features by applying FFT on the complete duration of each trial leading to single 3-channel image corresponding to each trial.
The second approach included dividing each trial to multiple time windows and extracting power features for each window separately leading to conser- vation of temporal information rather than averaging them out into single slice of activity map.

SINGLE FRAME CLASSIFICATION RESULTS:
 - We evaluated various configurations with different number of convolution and maxpool layers.
 - We followed the VGG architecture for selection of number of filters in each layer and grouping convolution layers with small receptive fields.
 - ConvNet based architectures to be superior to our baseline methods.
 - Most of the network parameters lie in the last two layers (fully connected and softmax) containing approximately 1 million parameters.
 - In VGG style network, the number of filters in each layer is selected in a way that size of the output remains the same after each stack (filter size × number of kernels).

The differences between topology-preserving and non-topology-preserving projections were mostly evident on the peripheral parts of the projected image.
In our experiments we observed slight improvement of classification error in using topology preserving projection over non-equidistant flat- tening projection (∼0.6%).
However, this observation could be dependent on the particular dataset and requires further exploration to conclude.
 - TODO: Generate images using a simple orthographic projection (onto the z=0 plane); maybe this will have a different outcome?

Using the equidistant projection approach helps with the interpretability of images and feature maps when visualizing the data.
 - Our claim is that mapping EEG data into a 2D image (specially with equidistant projections) leads to considerably better classification of cognitive load levels as compared to standard, non-spatial approaches that treat EEG simply as a collection of time series.

MULTI FRAME CLASSIFICATION RESULTS:
For the multi-frame classification, we used ConvNet with architecture D from previous step and applied it on each frame.
Using temporal convolution and LSTM significantly improved the classification accuracy.

For the model with temporal convolution, we found the network consisting of 32 kernels to outperform the one with 16 kernels.
 - A closer look at the accuracies derived for each individual, reveals that while both methods are achieving close to perfect classification accuracies for eight of participants, most of the differences originated from differences in accuracy for the remaining five individuals.
 - This observation motivated us to use a combination of temporal convolution and LSTM structures together in single structure which led to our best results on the dataset.

Our approach does not directly operate on raw EEG time- series, we drastically reduced the amount of required data by manually extracting power features from EEG.
Discovering complex temporal relationships such as those related to spectral properties in time-series using neural networks, is still an open question which has not been fully addressed.

ConvNets attain translation invariance through maxpooling which is a downsampling procedure in nature.
While this helps with creating invariant (with respect to space and frequency) feature maps in the deeper layers of ConvNet, it might also hurt the performance if the feature map size is reduced to a degree in which the regional activities cannot be distinguished from each other.
There is a trade-off between the degree of abstraction realized through layers of convolution and maxpooling and the level of detail kept in the feature maps.

ConvNets learn stack of filters which produce nonlinear feature maps maximizing the classification accuracy.
When trained on a pool of data containing multiple individuals, the network extracts features that are maximally informative considering the variability in the training set.

Performance of ConvNet+Maxpool is lower than ConvNet in single-frame setup.
Tem- poral maxpool selects the highest activation across the frames whereas features extracted in the single-frame approach are similar to average values over multiple frames.

Choosing the maximum value over multiple time frames is not necessarily the best practice when dealing with brain activity time series as it will potentially ignore the periods of inactivation in some cortical regions.
 - This effect partially observable when computing the average of activities over all the frames;
 - It also partially explains lower classification errors when temporal dynamic models (1D-conv and LSTM) are added to the network.






----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Paper: Convolutional Neural Network Based Approach Towards Motor Imagery Tasks EEG Signals Classification

A methodology based on Deep Convolutional Neural Networks (DCNN) for MI.
 - The proposed method first transforms the input EEG signals into images by applying the time-frequency (T-F) approaches.
 - The used T-F approaches are short-time-Fourier- transform (STFT) and continuous-wavelet-transform (CWT).
 - After T-F transformation the images of MI-tasks EEG signals are applied to the DCNN stage.

TODO: Include this within the report.
BCIs exploit the electrical activity in the brain of the handicapped patients who have lost their abilities due to severe injuries.
Brain activities related to MI tasks can be captured through various acquisition methods.
 - Among these procurement methods, Electroencephalogram (EEG) is considered as most substantial for non-invasive BCI systems, owing to its excellent temporal resolution, usability, non-invasiveness, portability, and low set-up costs.
 - System utilizes the brain activity of the disabled person and tries to assist and map their sensory- motor functions.
 - MI is one of the most frequently used mental strategy in BCI applications that deals with carrying out a motor task merely by thinking or imagining.
 - MI stands out from the rest of the BCI systems due to its biggest advantage of independence of any body movement, which itself resolves one of the major difficulty.

Improvement in the classification performance of MI signals remains to be a critical issue for the advancement of BCI systems.
 - The feasibility of these systems relies on their accuracy and efficiency.

A brain-machine interface (BMI) system has been designed using SVM classifier to regulate a robotic arm.

In the current framework, the utility of time-frequency representation (TFR) methods is examined on a CNN model, for the classification of the right hand (RH) and right foot (RF) MI tasks based EEG signals.
 - The considered TFR methods are short-time Fourier transform (STFT) continuous wavelet transform (CWT).

The CNN architecture that has been exploited is Alexnet. It is composed of a total of 8 layers, comprising 5 convolutional layers and 3 fully connected layers.
 - This work adopts transfer learning where the last two layers are discharged and replaced with new layers to solve the required purpose.

METHODOLOGY!

Relevant EEG signals are given as input to TF representations.
 - TFR techniques allow us to represent the 1-D EEG signals as 2-D images.
 - The TFR methods under consideration are STFT and CWT.
 - Both of these methods allow effective characterization of the signal in both time and frequency domains.
 - They basically transform the 1D EEG signals into 2D time-frequency images. They have already been applied in numerous problems for various purposes.


Following the transformation through TFR, MI-based EEG signals are displayed as images. These images are then fed into CNN for feature extraction and classification.

METHODOLOGY - DATASET:

The sampling frequency of recorded EEG signals is 1000 Hz, further it is down-sampled to 100 Hz, followed by bandpass filtering.
 - This was done to eliminate any contaminations and residues from the recorded EEG data.

METHODOLOGY - SHORT TIME FOURIER TRANSFORM (STFT)

A TF representation captures the variation of the spectral content of a signal over time.
 - One such most basic yet sophisticated technique is STFT;
 - It is an advanced Fourier analysis technique that presents a signal in a way that it can be completely estimated in both domains.
 - It is primarily used for the representation of properties of non-stationary signals.
 - STFT uses a window function to take off a portion of the time domain signal and then identifies various properties of the signal by applying Fourier transform onto the cut-out portion.
 - It can be visualized as symmetric, identical bandpass filters evenly spaced in frequency.

METHODOLOGY - CWT

Another such effective TFR method is CWT which provides an over-complete description of a signal.
 - It presents signals as a linear combination of principle or basis functions called wavelets, which are localized in time or space.
 - It allows localized signal content to be analyzed.
 - A wavelet is a finite- energy, zero-average function which indicates that it is an oscillating bandpass function.

METHODOLOGY - CNN

Deep learning aims at identifying various levels of distributed representations.
 - attempts to automate the signal classification process.

Deep learning algorithms are categorized as follows: CNN, Sparse Coding, Auto encoder, and Restricted Boltzmann Machines.
 - Inspired by the biological system, the CNN is counted as one of the most renowned technique of deep learning that involves vigorous training of multiple layers for feature extrac- tion and classification purpose.
    - It differs from the traditional algorithms in a manner where the network “learns” to extract features and classify, instead of manual operation.
    - It works on transfer learning where the model trained on one task is reused for the second task.
    - The basic idea behind this technique is the embodiment of spatial information between pixels of a given input image of the network.

CNN Architecture:
 - Embodies a special combination of an input layer, multiple hidden layers, and an output layer;
 - Each of these layers has a unique objective of processing the information received from the preceding layer;
 - The feature detection and extraction is carried out in the hidden layers using a series of convolution and pooling procedures.
 - These convolution and pooling layers are enclosed sequentially to formulate high- level features.
 - Therefore, it can be described as a hierarchical neural network composed of convolutional layers alternating with pooling layers, further succeeded by some fully connected layers.
    - Each layer is defined and the network is trained and used continuously.
    - These steps are repeated until the desired accuracy is obtained.
    - The whole network is trained by adopting the conventional back-propagation algo- rithm where the error is back propagated to boost the classification.

 - CONVOLUTION LAYER:
    - This layer is composed of a series of filters, also known as kernels or edge detectors whose coefficients are tuned in the course of the training procedure;
    - With a fixed number of filters in each layer, each individual filter is convolved transversely with the width and height of the input figure in the forward transmits.
    - The output of this layer is a two-dimensional feature map of that filter to detect the pattern;
    - This is followed by a Rectified linear unit (ReLu) where the non-linearity is increased in the network using a rectified function;

 - POOLING LAYER:
    - Succeeding the convolution layer is the pooling layer where the main objective is reduction of network parameters and dimensionality of feature maps while conserving useful information.
    - It creates a non- linear down-sampling by merging nearby values in the feature space through various operators.
    - The number of parameters and hence, the computation in the network is reduced by minimizing the spatial size of the input, allowing the network to go deeper.
    - It is translation invariant and also regulates over-fitting.
    - These hidden layers are succeeded by a fully connected layer that acts as the classifier for the formerly extracted features.

 - FULLY CONNECTED LAYER:
    - It feeds forwards the neural net- work by converting the 2D feature map into a 1D feature vector;
    - Scores are calculated for each category which is then converted into probabilities by a softmax layer.
    - Lastly, the classification layer allocates a probable class or category to the object on the basis of the algorithm.

With the recent surge of CNN schemes, some well-known deep learning CNN models or pre-trained networks have
emerged like LeNet, ZF Net, GoogLeNet, AlexNet and many more.
 - Transfer learning employs such pre-trained networks as reference model to train own model.
 - It involves the transfer of previously learned knowledge from one domain to another domain, for feature extraction and classification tasks.
 - It effectively performs deep learning by fine-tuning of model parameters.
    - This procedure is usually much faster than the conventional training of CNN model with random weights.
 - The most widely known out of all is AlexNet and left a huge impact on the machine learning domain.
    - This compelling CNN architecture is formed of 8 layers that comprise of 5 convolution layers and 3 fully connected layers.
    - It is trained on ImageNet and is employed for image classification.

RESULTS AND DISCUSSION

TODO: Using the former technique, use an AlexNet implementation.

At first, TF representation of the EEG signal is estimated using spectrograms and scalogram obtained through STFT and CWT, respectively.
 - This is assessed taking Hamming window under consideration.
 - he block length is taken as 120, therefore the window function is of length 120 units;
 - Amount of overlap allowed between adjacent blocks, i.e. the number of overlapping is limited to 100 units.
 - Each block is zero- padded to a length of 200 units.
 - All the simulation work is done considering sampling frequency as 100 Hz.
 - A CWT filter bank is created using Morse wavelet.
 - Observing the TFR of both, it can be concluded that CWT gives higher time-frequency resolution compared to STFT;
 - STFT provides a constant resolution over the entire spectrum.
 - As it was mentioned earlier, both STFT and CWT images were resized to 227 × 227 × 3.
 - These were used as input to DCNN model for feature extraction and classification purpose.
 - In DCNN based Alexnet model, first and second convolutional layers have 96 and 256 filters of size 11 × 11 × 3 and 5 × 5 × 48 respectively.
 - The max pooling layer follows the first two convolutional layers;
 - Layers 3, 4 and 5 follow on similar lines. The input to first fully connected layer is a 13 × 13 × 128 vector which is multiplied with a matrix to give an output of 1 × 2048 configuration.
 - Layers 7 and 8 follow on similar lines. Transfer learning is employed to fine-tune the pre-trained network Alexnet.
 - The last two layers are discharged for adaptation and replaced with new layers to learn features unique to the used dataset.
 - They are fine- tuned for MI tasks based EEG signal recognition where the number of classes is two (RH and RF). Then, the features abstracted from the trained images are used to classify the test images.

80% of input data has been utilized for training the Alexnet model and the residual 20% has been employed for testing the fine-tuned model.
 - The learning rate is set to 0.0001 for fine-tuning of the DCNN model. The maximum epoch is set to 15 and total 1830 iterations were carried out for the training procedure. Thus, 122 iterations are performed per epoch.

CWT yields better results as compared to STFT. The classification accuracy obtained using CWT is 0.66% higher as compared to STFT. Also, the error or the misclassification rate using CWT is mere 0.65% which is almost negligible.
 - CWT is more accurate and effective for the task of discrimination of RH and RF MI EEG signals. Though the specificity obtained using both the techniques is 100%, the sensitivity achieved is again better using CWT.
 - CWT is more reliable for identification of RH tasks.

Thus, utilization of T-F transformations for reconstruction of EEG signals into images combined with deep learning AlexNet model empowers as a robust approach. It is highly productive and can be definitely employed for the betterment of disabled people.

CWT images may have more distinctive features for EEG signals than the STFT method.